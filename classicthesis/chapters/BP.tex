\section{Belief Propagation} \label{BP}

The Warning Propagation algorithm is able to solve the decision problem and to compute valid assignments of SAT formulas. A different message passing method called the \emph{Belief Propagation} algorithm is able to determine the actual number of satisfying assignments and the fraction of those assignments where variables are restricted to fixed values. 

The messages sent in the BP algorithm are conditional probabilities $\mu \in [0, 1]$. The corresponding events are taken from the probability space built by all configurations of the  boolean variables $x_1, \ldots, x_n$. The used probability measure is a uniform distribution so for a SAT formula over $n$ variables each assignment has probability $2^{-n}$. \newline
This probability space can be restricted to only those configurations that satisfy a boolean formula $\mathcal{F}$. If there are $\mathcal{N}$ satisfying assignments the conditioned probability measure factorizes to $$P(X \; | \; X \text{ satisfies } \mathcal{F}) = \mathcal{N}^{-1} \prod_{a \in A} f_a(X)$$

where $A$ is the set of clauses in $\mathcal{F}$ and $f_a(X)$ the characteristic function of a clause $a$ which is $1$ if $X$ satisfies $a$ and $0$ if not. \newline
The fraction of satisfying assignments where for example $x_1 = 1$ and $x_4 = 0$ can now be viewed as a marginals of the factorized function. \newline

 Belief Propagation is a message passing technique for computing marginals of  factorized probability distributions. In this chapter the BP equations and their interpretations in the SAT context are explained and demonstrated on an example formula.

\subsection{Propagation Algorithm} \label{BPA}

The basic procedure of sending and updating messages between nodes of the function's factor graph stays the same as in WP. Instead of warnings, the BP messages are conditional probabilities. From the converged probabilities one can obtain marginals and the functions normalization constant $\mathcal{N}$. With this information one gets the total number of satisfying assignments as well as the number of assignments where a variable has a fixed value.

\subsubsection{Messages}
%The general idea behind Belief Propagation is to compute for each edge $u \rightarrow v$ a marginal 

The message $\mu_{a \rightarrow i}(x_i)$ sent from a factor $a$ to a variable $i$ is meant to be the probability that $a$ and all the factors behind $a$ are satisfied conditioned on $i$ taking the value $x_i$. 

%$\tau_{a \rightarrow i}$ is satisfied conditioned on $i$ taking the value $x_i$. 

The message $\mu_{i \rightarrow a}(x_i)$ sent in the opposite direction is the probability that $i$ takes the value $x_i$ in an assignment that satisfies $\tau_{i \rightarrow a}$.

\begin{example}
Let $\mathcal{F} = \underbrace{(x_1 \lor x_2 \lor \overline{x_3})}_a  \land \underbrace{(x_3 \lor x_4)}_b $.
\end{example}
\begin{figure}[h]
\centering

\begin{tikzpicture}[scale=0.8,transform shape]
   	\node[rectangle,draw=black, label = {$a$}, fill] (a) at (1,0) {$a$};
   	\node[rectangle,draw=black, label = {$b$}, fill] (b) at (4,0) {$a$};   
  \node[shape=circle,draw=black] (x1) at (0, 1) {$x_1$};
   
  \node[shape=circle,draw=black] (x2) at (0,-1) {$x_2$};
  \node[shape=circle,draw=black] (x3) at (2.5,0) {$x_3$};
  \node[shape=circle,draw=black] (x4) at (5.5,0) {$x_4$};
 


    
	\draw[-] (a) edge [right] node {} (x1);
	\draw[-] (a) edge [right] node {} (x2);
	\draw[-, dashed] (a) edge [right] node {} (x3);
	\draw[-] (b) edge [right] node {} (x3);
	\draw[-] (b) edge [right] node {} (x4);


\end{tikzpicture}
\end{figure}
The messages exchanged between $b$ and $x_3$ can be computed using only the definition: \newline
For example $\mu_{b \rightarrow 3}(1)$ is the probability that $(x_3 \lor x_4)$ is satisfied by an assignment where $x_3 = 1$. Its value is $1$ because each such assignment satisfies $b$. If $x_3 = 0$, only those assignments where $x_4 = 1$ will satisfy $b$ so $\mu_{b \rightarrow 3}(0) = 0.5$. \newline
Out of all 14 configurations satisfying $(x_1 \lor x_2 \lor \overline{x_3})$ there are $8$ where $x_3 = 0$ and $6$ where $x_3 = 1$, so $\mu_{3 \rightarrow b}(0) = \frac{4}{7}, \, \mu_{3 \rightarrow b}(1) = \frac{3}{7}$.
This method of counting desired configurations out of the whole configuration space is infeasible for larger graphs. Instead the messages are obtained by message passing.

\subsubsection{Update Rules}


\begin{wrapfigure}{r}{0.3\textwidth}

\begin{tikzpicture}[scale=0.8,transform shape]
   	\node[rectangle,draw=black, label = {$a$}, fill] (a) at (0,0) {$a$};
   	
\node[shape=circle,draw=black] (i) at (2,0) {$i$};   	
   	
   	\node[rectangle,draw=black, label = {$b_1$}, fill] (b1) at (3,1) {$a$};
   	\node[rectangle,draw=black, label = {$b_2$}, fill] (b2) at (3,-1) {$a$};

	\node[] (x1) at (-1,0) {};
	\node[] (x2) at (-1,1) {};
	\node[] (x3) at (-1,-1) {};

    \draw[dotted] (a) edge [right] node {} (x1);
    \draw[dotted] (a) edge [right] node {} (x2);
    \draw[dotted] (a) edge [right] node {} (x3);
	\draw[-] (a) edge [right] node {} (i);
	\draw[-] (i) edge [right] node {} (b1);
	\draw[-] (i) edge [right] node {} (b2);

\end{tikzpicture}
\end{wrapfigure}
$\mu_{b \rightarrow 3}(x)$, the message passed to $a$ tells the probability $i$ is $x_i = x$ considering only the factors on $i$'s side of the graph. \newline The node $i$ receives information about these factors through its neighbour vertices $b_i \neq a$. The incoming messages $\mu_{b_i \rightarrow i}(x)$ tell, if the factors are satisfied if $x_i = x$. \newline
Using Bayes theorem and the fact that $\mu_{b_i \rightarrow i}$ are conditionally independent given $x_i$, one can write the outgoing message  as a product of the incoming ones:
\begin{align*}
\mu_{i\rightarrow a}(x) = P(x_i = x \; | \; \tau_{i \rightarrow a} \text{ is sat})
&= \frac{P(x = x_i) P(x = x_i \; | \; \tau_{i \rightarrow a} \text{ is sat})}{P(\tau_{i \rightarrow a} \text{ is sat})} \\
&= \underbrace{\frac{P(x = x_i)}{P(\tau_{i \rightarrow a}\text{ is sat})}}_{C_{i \rightarrow a}} \prod_{b \neq a} \mu_{b \rightarrow i}(x) \\
\end{align*}

The factor $C_{i \rightarrow a}$ is the same for $x=0$ and $x=1$ so it suffices to compute the products for $x_i = 0$ and $x_i = 1$ and then to normalize so that $\mu_{i\rightarrow a}(0) + \mu_{i \rightarrow a}(1) = 1$. The usual notation to indicate the implicit existence of a formalization factor is $\mu_{i \rightarrow a}(x) \propto \prod \mu_{b \rightarrow i}(x)$. \cite{lecture} 


\begin{wrapfigure}{r}{0.3\textwidth}

\begin{tikzpicture}[scale=0.8,transform shape]
   	\node[rectangle,draw=black, label = {$a$}, fill] (a) at (2,0) {$a$};
   	
\node[shape=circle,draw=black] (i) at (0,0) {$i$};   	
   	
   	\node[shape=circle,draw=black] (b1) at (3,1) {$j_1$};
   	\node[shape=circle,draw=black] (b2) at (3,-1) {$j_2$};

	\node[] (x1) at (-1,0) {};
	\node[] (x2) at (-1,1) {};
	\node[] (x3) at (-1,-1) {};

    \draw[dotted] (i) edge [right] node {} (x1);
    \draw[dotted] (i) edge [right] node {} (x2);
    \draw[dotted] (i) edge [right] node {} (x3);
	\draw[-] (a) edge [right] node {} (i);
	\draw[-] (a) edge [right] node {} (b1);
	\draw[-] (a) edge [right] node {} (b2);

\end{tikzpicture}
\end{wrapfigure}

To compute $\mu_{a \rightarrow i}(x_i)$ one has to sum over all configurations $X$ of the variables in $a$ where $x_i = x$. If a configuration does not satisfy $a$, it contributes nothing to $\mu_{a \rightarrow i}(x_i)$. If it does, one has to compute the probability that it also satisfies the other factors in $\tau_{a \rightarrow i}$. The probability that a configuration where $j$ takes the value $x_j$ satisfies the factors behind the neighbour variable $j$ is $\mu_{j \rightarrow a}(x_j)$. Again these probabilities are independent and the probability for all factors is simply the product.

$$\mu_{a \rightarrow i}(x) = \sum_{ X = \{x_j (j \neq i), \; x_i = x\}} f_a(X) \prod_{j \neq i} \mu_{j \rightarrow a}(x_j)$$


\begin{example}
Now the messages from the first example can be computed with belief propagation.
% $$\mathcal{F} = \underbrace{(x_1 \lor x_2 \lor \overline{x_3})}_a  \land \underbrace{(x_3 \lor x_4)}_b $$
Since the formula's factor graph is a tree, the updates can be scheduled so that each message has to be computed only once.

\begin{itemize}
\item Level 0 (Leaves)

The leaves are all variable nodes. As they receive no incoming messages they will all send the same constant message. The empty product in the update rule evaluates to $1$, after normalization the values are \newline $\mu_{1 \rightarrow a}(0) = \mu_{1 \rightarrow a}(1) = 1/2$. The same holds for $2 \rightarrow a$ and $4 \rightarrow b$.

\item Level 1
The message of level $1$ are both sent from factors to variables: 
\begin{align*}
\mu_{a \rightarrow 3}(0) = &\mu_{1 \rightarrow a}(0)\mu_{2 \rightarrow a}(0) + \mu_{1 \rightarrow a}(0)\mu_{2 \rightarrow a}(1) + \mu_{1 \rightarrow a}(1) \mu_{2 \rightarrow a}(0) \\ + & \mu_{1 \rightarrow a}(1)\mu_{2 \rightarrow a}(1) = 1 \\
\mu_{a \rightarrow 3}(1) = &\mu_{1 \rightarrow a}(0)\mu_{2 \rightarrow a}(1) + \mu_{1 \rightarrow a}(1) \mu_{2 \rightarrow a}(0) + \mu_{1 \rightarrow a}(1)\mu_{2 \rightarrow a}(1) \\ = &3/4 \\
\mu_{b \rightarrow 3}(0) = &\mu_{4 \rightarrow b}(1) = 1/2 \\
\mu_{b \rightarrow 3}(1) = &\mu_{4 \rightarrow b}(0) + \mu_{4 \rightarrow b}(1) = 1
\end{align*}

\item Level 2
The unnormalized values for $3 \rightarrow b$ are $0.5$ and $1$. After normalization one gets $\mu_{3 \rightarrow b}(0) = \frac{0.5}{0.5 + 1} = \frac{1}{3}$ and $\mu_{3 \rightarrow b}(1) = \frac{2}{3}$. \newline
The values $\mu_{3 \rightarrow a}(0) = \frac{1}{1 + 0.75} = \frac{4}{7}$ and $\mu_{4 \rightarrow a}(1) = \frac{3}{7}$ are obtained in the same way.
%  \item Level 3

\end{itemize}
The values for the remaining 3 edges of level $4$ can be looked up in the tabular

\end{example}

\begin{figure}[h]
\begin{floatrow}
\centerfloat
\capbtabbox{%
  \begin{tabular}{ccc} \hline
  Edge & $\mu(0)$ & $\mu(1)$ \\ \hline
  $1 \rightarrow a$ & $1/2$ & $1/2$ \\
  $2 \rightarrow a$ & $1/2$ & $1/2$ \\ 
  $4 \rightarrow b$ & $1/2$ & $1/2$ \\ 
  $a \rightarrow 3$ & $1$   & $3/4$ \\ 
  $b \rightarrow 3$ & $0.5$ & $1$ \\ 
  $3 \rightarrow a$ & $1/3$ & $2/3$ \\ 
  $3 \rightarrow b$ & $3/7$ & $4/7$ \\ 
  $a \rightarrow 2$ & $5/6$ & $1$ \\ 
  $a \rightarrow 2$ & $1$ & $5/6$ \\ 
  $b \rightarrow 1$ & $4/7$ & $1$ \\ 
  \end{tabular}
}{}
%{%
%  \caption{A table}%
%}
\ffigbox{%
  \centering
  \centerfloat
\begin{tikzpicture}[scale=0.8,transform shape]
   	\node[rectangle,draw=black, label = left:{$a$}, fill] (a) at (0,1) {$a$};
   	\node[rectangle,draw=black, label = left:{$b$}, fill] (b) at (0,4) {$a$};   
  \node[shape=circle,draw=black] (x1) at (1, 0) {$x_1$};
   
  \node[shape=circle,draw=black] (x2) at (-1,0) {$x_2$};
  \node[shape=circle,draw=black] (x3) at (0,2.5) {$x_3$};
  \node[shape=circle,draw=black] (x4) at (0,5.5) {$x_4$};
 


    
	\draw[-] (a) edge [right] node {} (x1);
	\draw[-] (a) edge [right] node {} (x2);
	\draw[-, dashed] (a) edge [right] node {} (x3);
	\draw[-] (b) edge [right] node {} (x3);
	\draw[-] (b) edge [right] node {} (x4);


\end{tikzpicture}


}{%
  %\caption{A figure}%
}
\end{floatrow}
\end{figure}


The equations described above are the general BP update rules that can be applied not only to SAT problems but to any problem that is solved by belief propagation.
In \cite{survprob} and \cite{lecture} these standard rules are expressed in a different, but equivalent way to make computation easier. \newline
Instead of computing functions where $\mu_{i \rightarrow a}(0)$ and $\mu_{i \rightarrow a}(1)$ add up to $1$, one can also compute the value $\gamma_{i \rightarrow a}$, the probability that $x_i$ violates $a$. 
It can be expressed as $\gamma_{i \rightarrow a} = \left\{
  \begin{array}{@{}ll@{}}
    \mu_{i \rightarrow a}(0) & \text{if}\ J_a^i = 1 \\
    \mu_{i \rightarrow a}(1), &  \text{if}\ J_a^i = 0
  \end{array}\right\}$
  

In the original BP equation for $\mu_{a \rightarrow i}(x_i)$ one has to sum over all satisfying configurations of $a$'s variables $j \neq i$. Since there is only one configuration that does not satisfy $a$ it is easier to just compute the probability of this single configuration. The message $\delta_{a \rightarrow i}$ is the probability that all of $a$'s neighbours are in a state that violates $a$ (conditioned on $a$ still being satisfiable). The probabilities for these states are exactly the incoming messages $\gamma_{j \rightarrow a}$ which are conditionally independent.
$\delta_{a \rightarrow i} = \prod_{j \neq i} \gamma_{j \rightarrow a}$ is the first update equation for the modified BP version.

The more complicated part is to compute $\gamma_{i \rightarrow a}$ from incoming messages $\delta_{b \rightarrow i}$. The probability that $i$ violates $a$ conditioned on $\tau_{i \rightarrow a}$ being satisfied is again proportional to the probability  that $\tau_{i \rightarrow a}$ is satisfied conditioned on $i$ violating $a$. This is the case exactly if if no $b \neq a$ fixes $i$ to the value that would satisfy $a$.
$$\gamma_{i \rightarrow a} \propto P_{i \rightarrow a}^u := \prod_{b \in V_a^s(i)} (1 - \delta_{b \rightarrow i})$$
To get the real value of $\gamma_{i \rightarrow a}$ one has to normalize so that $P_{i \rightarrow a}^u$ and the probability for $i$ to satisfy $a$, $P_{i \rightarrow a}^s$, add up to $1$.

With these new update equations the overall update procedure is

\begin{lstlisting}[mathescape=true]
	Belief Propagation Algorithm
	
	0. Randomly initialize all message $\delta_{a \rightarrow i} \in_R [0, 1]$
	
	1. For $t=0$ to $t = t_{max}$
		1.1 Compute in random order for all edges (a, i)
		  $\delta_{a \rightarrow i} := \prod_{j \in V(a) \setminus i} \gamma_{j \rightarrow a}$		 
		where $\gamma_{j \rightarrow a} := \frac{P_{j\rightarrow a}^u}{P_{j\rightarrow a}^u + P_{j\rightarrow a}^s} = \frac{\prod_{b \in V_a^s(j)} (1 - \delta_{b \rightarrow j})}{\prod_{b \in V_a^s(j)} (1 - \delta_{b \rightarrow j}) + \prod_{b \in V_a^u(j)} (1 - \delta_{b \rightarrow j})}$
		 		
		1.2 If no $\delta_{a \rightarrow i}$ has changed more than $\epsilon$ goto 2.
	2. If $t = t_{max}$ return UN-CONVERGED, else return the generated warnings $\delta_{a \rightarrow i}^{\star}$
\end{lstlisting}

\subsection{Marginal Probabilities}

If the BP-Algorithm returns a set of converged messages one can obtain the single variable marginal $\mu_i = P(x_i = 1 \; | \mathcal{F} \text{ is satisfied})$ in the same way, the values $P_{i \rightarrow a}$ where defined: The normalized probability that $\mathcal{F}$ is satisfied conditioned on $x_i = 1$.
$$ \mu_i = \gamma_{j \rightarrow a} := \frac{\prod_{b \in V_-(i)} (1 - \delta_{b \rightarrow i})}{\prod_{b \in V_-(i)} (1 - \delta_{b \rightarrow i}) + \prod_{b \in V_+(i)} (1 - \delta_{b \rightarrow i})}$$

\subsection{Number of satisfying assignments}

The original normalization constant $\mathcal{N}$ can also be obtained from the converged messages. 